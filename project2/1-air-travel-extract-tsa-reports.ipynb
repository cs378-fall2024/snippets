{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Extract structured data from TSA reports"
      ],
      "metadata": {
        "id": "rj3jhwOUDMLW"
      },
      "id": "rj3jhwOUDMLW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helpful links:\n",
        "*   [Gemini SDK](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.generative_models)\n",
        "*   [GCS SDK](https://cloud.google.com/python/docs/reference/storage/latest)"
      ],
      "metadata": {
        "id": "SoT_mH1NDc7k"
      },
      "id": "SoT_mH1NDc7k"
    },
    {
      "cell_type": "code",
      "id": "pAewVjeSsr3oWghki94vLuvs",
      "metadata": {
        "tags": [],
        "id": "pAewVjeSsr3oWghki94vLuvs"
      },
      "source": [
        "!pip install vertexai\n",
        "!pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "import time\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "from pathlib import Path\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "from google.cloud import storage\n",
        "from google.cloud.storage import transfer_manager\n",
        "\n",
        "project_id = \"cs378-fa2024\"\n",
        "location = \"us-central1\"\n",
        "bucket_name = \"air-travel-data\"\n",
        "raw_folder = \"raw/tsa-traffic/raw/\"       # files which are downloaded by download_tsa_reports.py are written into this folder\n",
        "split_folder = \"raw/tsa-traffic/split/\"   # input location for the extract function\n",
        "llm_folder = \"raw/tsa-traffic/llm_text/\"  # output location for the extract function\n",
        "model_name = \"gemini-1.5-flash-001\"       # Note: Gemini Flash doesn't support the schema response option\n",
        "prompt = \"convert the file to json format. Return the date, hour of day, airport code, airport name, city, state, checkpoint, and customer traffic.\"\n",
        "\n",
        "def split_documents():\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    blobs = storage_client.list_blobs(bucket_name, prefix=raw_folder)\n",
        "\n",
        "    for blob in blobs:\n",
        "\n",
        "        if blob.name == raw_folder:\n",
        "            continue\n",
        "\n",
        "        source_filename = blob.name\n",
        "        print(\"downloading\", source_filename)\n",
        "        blob.download_to_filename(source_filename)\n",
        "\n",
        "        start_page = 1\n",
        "        pdf_reader = PdfReader(blob.name)\n",
        "        pdf_writer = PdfWriter()\n",
        "\n",
        "        for page_num, page_data in enumerate(pdf_reader.pages, 1):\n",
        "            pdf_writer.add_page(page_data)\n",
        "            remainder = page_num % 500\n",
        "\n",
        "            if (page_num % 500 == 0):\n",
        "                file_name = blob.name.split(\".pdf\")[0].replace(raw_folder, split_folder)\n",
        "                file_path = f\"{file_name}_{start_page}_{page_num}.pdf\"\n",
        "                print(\"trying to write\", file_path)\n",
        "\n",
        "                with open(file_path, \"wb\") as out:\n",
        "                    pdf_writer.write(out)\n",
        "                    pdf_writer = PdfWriter()\n",
        "                    print(\"wrote local file\", file_path)\n",
        "\n",
        "                # move the start page marker\n",
        "                start_page = page_num + 1\n",
        "\n",
        "        # write remaining file\n",
        "        if page_num > start_page:\n",
        "            file_path = f\"{file_name}_{start_page}_{page_num}.pdf\"\n",
        "            print(\"trying to write last file\", file_path)\n",
        "\n",
        "            with open(file_path, \"wb\") as out:\n",
        "                pdf_writer.write(out)\n",
        "                print(\"wrote last local file\", file_path)\n",
        "\n",
        "\n",
        "def copy_to_GCS(local_folder, gcs_folder, file_extension):\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    directory_as_path_obj = Path(local_folder)\n",
        "    file_paths = directory_as_path_obj.rglob(file_extension)\n",
        "    relative_paths = [path.relative_to(local_folder) for path in file_paths]\n",
        "    string_paths = [str(path) for path in relative_paths]\n",
        "    print(\"Found {} files.\".format(string_paths))\n",
        "\n",
        "    results = transfer_manager.upload_many_from_filenames(bucket, string_paths, source_directory=local_folder, blob_name_prefix=gcs_folder, max_workers=5)\n",
        "\n",
        "    for name, result in zip(string_paths, results):\n",
        "\n",
        "        if isinstance(result, Exception):\n",
        "            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n",
        "        else:\n",
        "            print(\"Uploaded {} to {}.\".format(name, bucket.name))\n",
        "\n",
        "\n",
        "def extract():\n",
        "\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "    model = GenerativeModel(model_name)\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    blobs = storage_client.list_blobs(bucket_name, prefix=split_folder)\n",
        "\n",
        "    for blob in blobs:\n",
        "\n",
        "        if blob.name == split_folder:\n",
        "            continue\n",
        "\n",
        "        # check if file has already been processed\n",
        "        filename = blob.name.replace(split_folder, llm_folder).replace(\".pdf\", \".txt\")\n",
        "\n",
        "        f = Path(filename)\n",
        "        if f.exists():\n",
        "            print(f\"{filename} already exists\")\n",
        "            continue\n",
        "\n",
        "        print(f\"extracting {blob.name}\")\n",
        "        file_content = Part.from_uri(f\"gs://{bucket_name}/{blob.name}\", \"application/pdf\")\n",
        "        resp = model.generate_content([file_content, prompt])\n",
        "        resp_str = str(resp.candidates[0].text).replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "        print(\"got resp from LLM\")\n",
        "\n",
        "        f = open(filename, \"w\")\n",
        "        f.write(resp_str)\n",
        "        f.close()\n",
        "        print(\"wrote file\", filename)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    split_documents() # split pdf documents due to large size\n",
        "    copy_to_GCS(split_folder, split_folder, \"*.pdf\") # copy split documents to GCS\n",
        "    extract() # call LLM and extract attributes from documents\n",
        "    copy_to_GCS(llm_folder, llm_folder, \"*.txt\") # copy LLM output to GCS\n"
      ],
      "metadata": {
        "id": "lGwBoioMDNyu"
      },
      "id": "lGwBoioMDNyu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "shirley.cohen (Sep 6, 2024, 10:47:26â€¯AM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}